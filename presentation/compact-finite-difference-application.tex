

\section{Application to compact finite differences}

\begin{frame}
\frametitle{Application to compact finite difference schemes}
\begin{itemize}
\item Compact finite difference evaluation involves two steps:
\begin{enumerate}
\item Evaluation of the right hand sides
\begin{itemize}
    \item easily performed on the GPU
\end{itemize}
\item Solving the tridiagonal systems for all the grid lines
\end{enumerate}
\item Current efforts:
\begin{itemize}
    \item Tutkun et al. (2012)
    \item Esfahanian et al. (2014)
\end{itemize}
\item Typical DNS problems too large to fit on a single GPU
\item Need a strategy for multiple GPUs
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallelization strategy: multiple GPUs on a single node}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Method used by Sakharnykh et al.
\item Decompose domain into \emph{subdomains};
    each GPU assigned a subdomain
\item Single subdomain used along derivative direction
\item No coordination between GPUs required
\item For other coordinate directions,
    transpose the data (physically or logically)
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[width=100px]{img/compact-shared-gpu.eps}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Parallelization strategy: multiple GPUs on distinct nodes}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item Single subdomain used along derivative direction
\item No coordination between GPUs required
\item \textbf{Physical} global transpose required
\item Subdomains can become impractically slender
\item Scalable approach needs a distributed tridiagonal solver
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
    \only<1>{\includegraphics[width=100px]{img/compact-distributed-restricted.eps}}
    \only<2>{\includegraphics[width=150px]{img/compact-restricted-transpose.eps}}
    \only<3>{\includegraphics[width=120px]{img/compact-distributed-all.eps}}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Distributed tridiagonal solver}

Solving a tridiagonal system of size
$mP$, split among $P$ processes (Mattor et al., 1995):

\footnotesize
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{enumerate}
\item Each process solves the local systems:
    \begin{align*}
        A^px_r^{p} &= r_p \\
        A^pu^p &= \{-a_1^p, 0, 0, 0, \hdots\} \\
        A^pl^p &= \{0, 0, 0 \hdots, -c_m^p\}
    \end{align*}
\item Construct and assemble a ``reduced'' system
    of the form:
    \scalebox{0.6}{
        \vbox{
    \begin{align*}
     \begin{bmatrix}
l^1_m & -1 \\
-1    & u^2_1 & l^2_1 \\
  & u^2_m & l^2_m & -1 \\
  &       & -1    & u^3_1 & l^3_1 \\
  &       &       & u^3_m & l^3_m  & -1 \\
  &       &       &       & \ddots & \ddots & \ddots \\
  &       &       &       &        & -1     & u^P_1
\end{bmatrix}
\begin{bmatrix}
\beta^1 \\
\alpha^2 \\
\beta^2 \\
\alpha^3 \\
\beta^3 \\
\vdots \\
\alpha^P
\end{bmatrix}
=
\begin{bmatrix}
x_{r,m}^1 \\
x_{r,1}^2 \\
x_{r,m}^2 \\
x_{r,1}^3 \\
x_{r,m}^3 \\
\vdots \\
x_{r,1}^P \\
\end{bmatrix}  
        \end{align*}}}
\item Local part of solution given by:
\begin{align*}
x^p = x_r^p + \
    \alpha^p u^p + \beta^p l^p
\end{align*}
\end{enumerate}
\end{column}
\begin{column}{0.5\textwidth}
\centering
\includegraphics[width=120px]{img/dividing-tridiagonal.eps}
\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Compact finite difference on multi-GPU system}
\footnotesize
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{enumerate}
\item Computing RHS for all local grid points - \textbf{requires communication}
\begin{itemize}
    \footnotesize
    \item halo swaps with neighbouring GPUs
    \item pointwise stencil
\end{itemize}
\item Solving local tridiagonal systems - no communication
\item Assembling and solving reduced tridiagonal systems - \textbf{requires communication}
\begin{itemize}
    \footnotesize
    \item global communication along line of GPUs
    \item produces interleaved tridiagonal sysems - pThomas algorithm
\end{itemize}
\item Summing solutions from steps 2 and 3 - no communication
\begin{itemize}
    \footnotesize
    \item pointwise sum
\end{itemize}
\item For other coordinate directions: \emph{local} permutation of data
\end{enumerate}
\end{column}
\begin{column}{0.5\textwidth}
\only<1>{\includegraphics[width=175px]{img/rhs-halos.eps}}
\only<2>{\includegraphics[width=175px]{img/constructing-reduced-system.eps}}
\end{column}
\end{columns}
\end{frame}


