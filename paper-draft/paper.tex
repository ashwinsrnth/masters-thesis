\documentclass{elsarticle}

\usepackage{amsmath,bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{rotating}

\begin{document}

\begin{frontmatter}
\author{Author names}
\title{A fast GPU solver for near-Toeplitz tridiagonal systems}
\maketitle

\begin{abstract}
    In this paper, we consider the solution of linear
    systems of the form $A\bm{x} = \bm{d}$,
    where $A$ is \emph{near-Toeplitz} tridiagonal.
    These systems appear in numerical schemes
    such as
    Alternating Direction Implicit (ADI) methods,
    compact finite differences,
    and one-dimensional ordinary
    and partial differential equations.
    We develop a GPU solver that takes advantage
    of the known structure of the matrix
    to reduce the number of computations
    and improve memory access.
    Our solver is able to outperform
    the NVIDIA CUSPARSE and
    multi-core Intel MKL tridiagonal solvers
    (on up to 16 cores).
    We also describe the implementation of a multi-GPU solver
    using MPI and CUDA,
    and show scalability for up to 64 GPUs
    for a 3-D compact finite difference application.
\end{abstract}

\end{frontmatter}
    
%---------------------------------------------------------------------%
\section{Introduction}
%---------------------------------------------------------------------%

The focus of this paper is the solution of
linear systems of the form $A\bm{x} = \bm{d}$,
where $A$ has the following structure

\begin{equation} \label{eqn:toeplitz-matrix}
A = 
\begin{pmatrix}
     b_1 & c_1  \\
     a_0 & b_0  &  c_0  \\
         & a_0  &  b_0 &  c_0  \\
         &      &  a_0 &  b_0 &  c    \\
         &      &      &      &  \ddots \\
         &      &      &      &     &  \ddots  \\
         &      &      &      &     &  a_n  &  b_n
\end{pmatrix}
\end{equation}
%
where, in general
\begin{align*}
    & a_0 \neq a_n & \\
    & b_1 \neq b_0 \neq b_n &\\
    & c_1 \neq c_0 &
\end{align*}

We refer to matrices with the specific structure of $A$
as \emph{near-Toeplitz tridiagonal matrices},
and the linear system $A\bm{x}=\bm{d}$ as \emph{near-Toeplitz tridiagonal systems}.

Such tridiagonal systems
appear in several numerical schemes
in Computational Fluid Dynamics (CFD)
such as
compact finite differences \cite{lele1992compact},
Alternating Direction Implicit (ADI) methods \cite{1955ADI}, and
numerical solutions of one-dimensional differential equations.
In such applications,
the evaluation of the tridiagonal systems constitutes
a significant portion of the runtime.
Recently, there has been considerable interest in the CFD community
in developing general tridiagonal solvers that
exploit the massive parallelism afforded by
Graphics Processing Units (GPUs)
to speed up the computations
\cite{tutkun2012gpu}
\cite{esfahanian2014efficient}
\cite{GoSt11CR}.
The performance of tridiagonal solvers on the GPU
has been shown to depend on several factors,
such as
choice of algorithm and the associated complexity,
thread utilization,
use of the available memory hierarchy,
and synchronization and control costs
\cite{Zhang2010FTS}.
Many approaches described in the literature
seek to make trade-offs between these factors
to arrive at a solver that best fits the requirements.
But to our knowledge,
the algorithms proposed so far
are applicable to general tridiagonal systems,
and make no attempt to take advantage of
the known structure of the tridiagonal matrix.
The contribution of this paper is twofold:
First, we describe the implementation of a GPU solver
for the specific case of near-Toeplitz systems
that uses the known structure to reduce
computations and memory accesses.
Secondly, we describe the implementation for
for multi-GPU clusters using MPI and CUDA,
and apply it to the evaluation of compact finite differences.

This paper is organized as follows:
Section \ref{sec:preliminaries}
describes algorithms for solving tridiagonal systems on GPUs.
The cyclic reduction algorithm in particular, is discussed in detail,
and issues with its implementation on GPUs is discussed.
Section \ref{sec:proposed-algorithm}
describes our proposed solver for near-Toeplitz tridiagonal systems.
Section \ref{sec:gpu-implementation}
discusses the implementation of the algorithm on GPUs.
Section \ref{sec:results-single-gpu}
provides an overview of the performance of our solver,
and compares it with other implementations.
Section \ref{sec-compact-finite-differences}
describes the implementation on multiple GPUs,
and application to the solution of compact finite differences.

%---------------------------------------------------------------------%
\section{Preliminaries} \label{sec:preliminaries}
%---------------------------------------------------------------------%

\subsection{Cyclic reduction} \label{subsec:cyclic-reduction}

\begin{equation} \label{eqn:general-tridiagonal-system}
\begin{pmatrix}
     b_1 & c_1  \\
     a_2 & b_2  &  c_2  \\
         & a_3  &  b_3 &  c_3  \\
         &      &  a_4 &  b_4 &  c_4  \\
         &      &      &      &  \ddots \\
         &      &      &      &     &  \ddots  \\
         &      &      &      &     &  a_n  &  b_n
\end{pmatrix}
\begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3 \\
    x_4 \\
    \vdots \\
    \vdots \\
    x_n
 \end{bmatrix}
=
\begin{bmatrix}
   d_1 \\
   d_2 \\
   d_3 \\
   d_4 \\
   \vdots \\
   \vdots \\
   d_{n}
\end{bmatrix}
\end{equation}

Cyclic reduction is applicable to the solution of
general linear systems of the form $A\bm{x} = \bm{d}$,
where $A$ is a tridiagonal matrix with diagonals
$\bm{a}$, $\bm{b}$ and $\bm{c}$
(Equation \ref{eqn:general-tridiagonal-system}).
The algorithm consists of two phases:
\emph{forward reduction} and \emph{backward substitution}
\ref{fig:cyclic-reduction}.
In the forward reduction phase,
every even-indexed equation $i$
is expressed as a
linear combination of equations $i$, $i-1$ and $i+1$,
yielding a new tridiagonal system of
$n/2$ equations in $n/2$ unknowns
(Equations \ref{eqn:forward-reduction-1} - \ref{eqn:forward-reduction-4}).
The process is repeated until a system of
2 equations in 2 unknowns is left.

\begin{align} 
& a^{\prime}_i = -a_{i-1}k_1 \
    \label{eqn:forward-reduction-1}& \\
& b^{\prime}_i = b_i - c_{i-1}k_1 - a_{i+1}k_2 \
    \label{eqn:forward-reduction-2}& \\
& c^{\prime}_i = -c_{i+1}k_2 \
    \label{eqn:forward-reduction-3}& \\
& d^{\prime}_i = d_i - d_{i-1}k_1  - d_{i+1}k_2 \
    \label{eqn:forward-reduction-4}&
\end{align}
%
where,

\begin{align}
& k_1 = \frac{a_i}{b_{i-1}} \label{eqn:k1-update}& \\
& k_2 = \frac{c_i}{b_{i+1}} \label{eqn:k2-update}&
\end{align}

The 2-by-2 system of equations is solved trivially,
yielding $x_n$ and $x_{n/2}$.
In the backward substitution phase,
every odd-indexed unknown $x_i$ is solved for by
substituting the known values of $x_{i-1}$ and $x_{i+1}$
(Equation \ref{eqn:backward-substitution}) .

\begin{align} \label{eqn:backward-substitution}
x_i = \frac{d^{\prime}_i - a^{\prime}_ix_{i-1} - \
    c^{\prime}_ix_{i+1}}{b^{\prime}_i}
\end{align}
%
For the last index $i=n$,
the forward reduction step is instead:
\begin{align} \label{eqn:forward-reduction-last}
    & a^{\prime}_n = -a_{n-1}k_1 & \\
    & b^{\prime}_n = b_n - c_{n-1}k_1 & \\
    & d^{\prime}_n = d_n - d_{n-1}k_1&
\end{align}
%
And for $i=1$, the backward substitution step is instead:
\begin{align} \label{eqn:backward-substitution-first}
x_1 = \frac{d^{\prime}_1 - c^{\prime}_1x_{2}}{b^{\prime}_1}
\end{align}

Thus, in the best case ($n$ parallel processors),
cyclic reduction requires 
$2log(n) - 1$ steps.
For even moderately large $n$,
this is significantly smaller than
the $2n$ steps required by the classical Thomas algorithm.
This makes cyclic reduction a good fit
for massively parallel architectures like GPUs.

\begin{figure}[h!]
\begin{center}
\includegraphics[height=250pt]{img/cyclic-reduction.eps}
\end{center}
\caption{Cyclic reduction}
\label{fig:cyclic-reduction}
\end{figure}

\subsection{GPU architecture} \label{subsec:gpu-architecture}

Writing applications that effectively exploit the GPU
requires an understanding of the underlying architecture.
We provide the pertinent details here,
and refer to \cite{GPUcomputingera} for a more complete picture.
Our tests are performed on the NVIDIA Tesla K20 and K40 GPUs,
built on the ``Kepler'' architecture,
an overview of which is available at \cite{Keplerwhitepaper}.

Applications that use the GPU include special
pieces of code that are executed on the GPU,
referred to as \emph{kernels}.
In C, for example, kernels are written as functions,
and are called with (almost) the same conventions.
A kernel is executed in parallel by several lightweight threads,
which are organized into a \emph{grid} of \emph{thread blocks}
(or just \emph{blocks}).
As a requirement, thread blocks must be able to run
independent of each other,
in series or in parallel.

From the hardware perspective, an NVIDIA GPU may be viewed primarily as
a collection of so-called Streaming Microprocessors (SMs).
When a kernel is launched (with a specified number of thread blocks),
each block is assigned to an SM.
Threads within a thread block can execute concurrently on the SM,
and an SM can execute several thread blocks concurrently.
The number of thread blocks that an SM can execute concurrently
is limited by a number of factors,
and maximizing this number is often key to obtaining good performance.

Every thread in every block has access to so-called \emph{global memory},
which can also be accessed by the CPU (or \emph{host}) via the PCI-e bus.
Further,
threads within a thread block have access
to a common, fast, limited \emph{shared memory}.
The contents of shared memory are managed by the kernel code,
and shared memory is often viewed as an explicitly controlled cache.
Each thread also has private \emph{local memory},
and access to extremely fast registers.
Unlike CPUs, the GPU has a large number of registers---a
thread executing a kernel will
typically attempt to store the variables defined in registers,
before spilling over to local memory, which is much slower.

Thread access to global memory is slow,
and is especially inefficient when
successive threads in a block
access locations that are far apart in memory---known
as \emph{uncoalesced} memory access.
Shared memory access can be expected to be
much faster than global memory---however,
the amount of shared memory per SM is limited
(48 KiB for current GPUs).
The amount of shared memory actually used by each block,
can thus limit the number of blocks that
concurrently run on each SM---known as the \emph{occupancy}.
The number of registers per SM is also limited,
and the use of registers can similarly affect occupancy.
Finally, memory transfers between the CPU (host) and GPU (device)
are extremely expensive, limited by the bandwidth of the PCI-e bus.

\subsection{Cyclic reduction implementation on GPU}

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/forward-reduction-step.eps}
\end{center}
\caption{Updating $\bm{b}$ in the first forward reduction step}
\label{fig:forward-reduction-step}
\end{figure}

As described in Section \ref{subsec:gpu-architecture},
GPU instructions are executed concurrently
by several threads and
threads are organized into thread blocks.
In most GPU implementations of cyclic reduction,
blocks are assigned to tridiagonal systems
(when solving multiple systems),
and threads within a block 
are assigned to equations, or \emph{indices}.
(Figure \ref{fig:gpu-mapping})
During the forward reduction phase,
the threads assigned to each even index $i$
compute the coefficients
$a_i^\prime$, $b_i^\prime$ and $c_i^\prime$.
In practice, the coefficients are updated \emph{in-place}
(Figure \ref{fig:forward-reduction-step} shows the updates
to the coefficient array $\bm{b}$ in the first forward reduction step).

In each step of forward reduction,
a thread accesses values from the coefficient arrays
$\bm{a}$, $\bm{b}$ and $\bm{c}$ in a \emph{strided} fashion.
At every subsequent step,
this stride is doubled, while the number of active threads is halved.
In the backward substitution phase,
the strides are halved at each step,
while the number of active threads is doubled.
The long-strided memory accesses towards the end
of the forward reduction
and the beginning of backward substitution
motivates the use of shared memory---strided global memory access is,
in general, associated with performance penalties.
But the limited amount of shared memory places restrictions
on the size of tridiagonal systems that can be solved.
Further,
the memory access pattern in
cyclic reduction leads to \emph{bank conflicts}
\cite{Zhang2010FTS}.
Zhang et al. \cite{Zhang2010FTS} propose a
\emph{hybrid} solver
that uses both cyclic reduction and parallel cyclic reduction
to reduce bank conflicts.
G{\"o}dekke et al. \cite{GoSt11CR}
use a method of separately storing
the even and odd indexed equations
to arrive at a bank-conflict free solver
at the cost of additional shared memory usage.
Shared memory usage is also associated with
lowered GPU occupancy (Section \ref{subsec:gpu-architecture}).
Davidson et al. \cite{davidson2011register}
describe the method of
\emph{register packing}---performing more computations
on \emph{registers}, rather than shared memory---as
a means to reduce shared memory usage in cyclic reduction.
More recently, Esfahanian et al. \cite{esfahanian2014efficient}
describe an implementation of cyclic reduction
that works entirely in global memory,
using additional kernels to reorder data so as to
coalesce memory access.

%----------------------------------------------------------------------%
\section{Proposed algorithm} \label{sec:proposed-algorithm}
%----------------------------------------------------------------------%

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/gpu-mapping.eps}
\end{center}
\label{fig:gpu-mapping}
\caption{Mapping work to blocks and threads:
systems are mapped to blocks and
indices are mapped to individual threads}
\end{figure}

The method is based on precomputing elements 
appearing in the forward reduction phase
so that they can be reused for multiple right hand sides.
While this can be done for general tridiagonal systems,
the applicability to near-Toeplitz systems is
of particular interest due to the lower storage costs.
In Figure \ref{fig:forward-reduction-step},
we list the updates to
$b_i$, for $i=2, 4, 6 ... n$
during the first forward reduction step
for a general tridiagonal matrix.
Similar equations can be written describing
the updates to $a_i$ and  $c_i$.

When the matrix $A$ is near-Toeplitz tridiagonal,
if we substitute in these equations:

\begin{itemize}
    \item [] $a_2 = a_3 = a_4 = \hdots \equiv a_0$
    \item [] $b_2 = b_3 = b_4 = \hdots \equiv b_0$
    \item [] $c_2 = c_3 = c_4 = \hdots \equiv c_0$
\end{itemize}
%
we observe that the resulting coefficients
$a_i^\prime$, $b_i^\prime$ and $c_i^\prime$
correspond to the coefficients of a tridiagonal matrix with
exactly the near-Toeplitz structure of $A$.
This form-preserving property of cyclic reduction
has been reported for block Toeplitz tridiagonal systems
by Bini et al.

At each reduction step,
for every index $i$,
the coefficients $a_i^\prime$, $b_i^\prime$, $c_i^\prime$
and auxiliary variables $k_1$ and $k_2$
(hereby denoted as $k_{1,i}$ and $k_{2,i}$) are identical,
except for those influenced by the
boundary coefficients of the previous step.
It is possible to \emph{precompute} these values
and store them in separate arrays.
In Figure \ref{fig:cyclic-reduction-precomputing},
it is seen that the space required for storing each of the
forward reduction coefficients
$a_i^\prime$, $b_i^\prime$, $c_i^\prime$,
$k_{1,i}$ and $k_{2,i}$
for a near-Toeplitz matrix
is, at most, $log_2n-2 + 2(log_2n-1)$.

\begin{figure}[h!]
\begin{center}
\includegraphics[height=250pt]{img/cyclic-reduction-precomputing.eps}
\end{center}
\caption{Storing forward reduction coefficients
    from each reduction step $m$}
\label{fig:cyclic-reduction-precomputing}
\end{figure}

By precomputing and storing the forward reduction
coefficients,
the $m^{th}$ forward reduction step is reduced to:

\begin{equation}
d^{\prime}_i = d_i - d_{i-1}k_1^{m}  - d_{i+1}k_2^{m}
\label{eqn:precomputed-forward-reduction-step}
\end{equation}
%
And the $m^{th}$ backward substitution step is:

\begin{equation}
x_i = \frac{d^{\prime}_i - a^mx_{i-1} - \
    c^{m}x_{i+1}}{b^m}
\label{eqn:precomputed-backward-substitution-step}
\end{equation}
%
where $a^m$, $b^m$, $c^m$, $k_1^m$ and $k_2^m$
are the appropriate values
from the precomputed coefficient arrays.
In practice,
the backward substitution step
can overwrite the right-hand side vector $\bm{d}$
with the solution values.

%----------------------------------------------------------------------%
\section{GPU implementation} \label{sec:gpu-implementation}
%----------------------------------------------------------------------%

In this section, we describe the implementation of a GPU solver
for solving a given near-Toeplitz tridiagonal system
for $N_{rhs}$ right hand sides.
We develop two approaches based on the common idea
of precomputed forward reduction coefficients---one that leverages
the GPU's \emph{shared memory},
and the other working directly with global memory.

Following the algorithm described in \ref{sec:proposed-algorithm},
the forward reduction coefficients
are first computed and loaded into global memory.
The right hand sides are stored in a single
contiguous region global memory.
In both approaches,
we map each of the $N_{rhs}$ tridiagonal systems
to a thread block,
and equations or \emph{indices} of a system
to individual threads within a block
(Figure \ref{fig:gpu-mapping}).

\begin{figure}[h!]
\begin{center}
\includegraphics[width=400pt]{img/global-and-shared.eps}
\end{center}
\caption{Shared memory (top) and Global memory (bottom) implementations}
\label{fig:global-and-shared}
\end{figure}

\subsubsection*{Global memory}

This implementation works entirely on the GPU's global memory,
Here, we define two kernels - one for the forward reduction step,
and the other for the backward substitution step.
Each kernel is called $log_2(n)-1$ times.
An extra call to the
forward reduction kernel performs
the two-by-two solve.
At each step,
the size of the grid block is determined by the \emph{stride}
between elements accessed at that step.
For the forward reduction phase,
we use $n/2$ threads per block for the first step,
$n/4$ threads for the second step, and so on.
The pattern is reversed for the backward substitution phase,
beginning with 2 threads per block for the first step.
Although this ensures that there are no inactive threads
at any stage,
the occupancy of the GPU is still very low
during the end of forward reduction
and the end of backward substitution.

The precomputed coefficient arrays and right hand
are accessed by the kernels from global memory.
The kernel suffers from
strided memory access for the right-hand side,
but the precomputed coefficient values are accessed
without major coalescing problems.
Further, by precomputing the forward reduction coefficients,
we greatly reduce the number of computations
(and thus the number of uncoalesced memory accesses).

\subsubsection*{Shared memory}

In the shared memory approach (Figure \ref{fig:global-and-shared}),
we launch a single kernel to perform the entire
cyclic reduction solve.
The kernel is launched with $n/2$ threads per block.
Each thread block allocated a block of shared memory of size $n/2$.
Each thread performs the first reduction step
(Equation \ref{eqn:precomputed-forward-reduction-step})
by accessing the required values
$d_i$, $d_{i-1}$ and $d_{i+1}$ from global memory,
storing the result in shared memory.
In subsequent reduction steps,
$d_i$, $d_{i-1}$ and $d_{i+1}$
are accessed from shared memory,
avoiding the uncoalesced global memory accesses
seen in the global memory implementation.
In each back substitution step,
threads overwrite the existing values in shared memory
with the values of the solution.
In the final step,
shared memory is filled completeley
with the even-indexed solution values.
Each thread then computes an odd-indexed solution value,
storing it directly in global memory
and copies the even-indexed solution value
from shared memory to global memory.
The entire solution is done in a single kernel launch
to hide the latency of
memory transfers between global and shared memory.
Explicit synchronization between the threads of a block
is required within the kernel at the end of each step.

The shared memory implementation suffers from two major issues:
first, the number of active threads is halved at each forward reduction step,
(and subsequently doubled at each backward reduction step).
Synchronization between threads of a block is necessary at each step.
Thus, a significant portion of the kernel execution time is
spent by idle threads waiting for active threads to complete execution.
Secondly, the strided access to the
right-hand side values leads to \emph{bank-conflicts}.
However, the number of bank conflicts is significantly smaller
than in cyclic reduction implementations for general tridiagonal systems,
due the reduced number of computations performed.

%----------------------------------------------------------------------%
\section{Results} \label{sec:results-single-gpu}
%----------------------------------------------------------------------%

\begin{figure}[h!]
\begin{center}
\includegraphics[width=300pt]{fig/bench-2d.eps}
\caption{Relative solver performance for 2-D problems}
\label{fig:bench-2d}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=300pt]{fig/bench-3d.eps}
\caption{Relative solver performance for 3-D problems}
\label{fig:bench-3d}
\end{center}
\end{figure}

In this section we present a performance overview
of our Near-Toeplitz tridiagonal solver (NEATO)
against a multi-threaded Intel MKL solver and
the CUSPARSE GPU solver
(\texttt{dgtsv} and \texttt{dgtsvStridedBatch} respectively).
The CPU code is compiled with the Intel C compiler
and run on up to 16 cores of the
Intel Xeon Processor E5-2670 v2 (25M Cache, 2.50 GHz).
GPU code is compiled with the CUDA toolkit (version 6.5.14),
and run on the
NVIDIA Tesla K20 Accelerator.
\texttt{-O2} level optimizations are turned on for both
CPU and GPU code.

\subsection{Global memory v/s shared memory solver performance}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/global-vs-shared-2d.eps}
\caption{Comparison of global memory and shared
    memory implementations of NEATO (2D problems)}
\label{fig:global-vs-shared-2d}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=1.0\linewidth]{fig/global-vs-shared-3d.eps}
\caption{Comparison of global memory and shared memory
    implementations of NEATO (2D problems)}
\label{fig:global-vs-shared-3d}
\end{center}
\end{figure}

In Figures \ref{fig:global-vs-shared-2d} and \ref{fig:global-vs-shared-3d},
we report the performance of the two solvers for the case
$N_{rhs} = n$ and $N_{rhs} = n^2$.
These cases correspond to tridiagonal systems
arising in 2-D and 3-D problems respectively.
We note that the shared memory implementation
offers better performance in nearly all cases.
However, the relative speedup from using
shared memory diminishes with increasing problem size.
For larger problem sizes,
the reduced occupancy and increase in number of inactive threads
leads to poor shared memory performance.

\subsection{Comparison with Intel MKL and CUSPARSE solvers}

In Figures \ref{fig:bench-2d} and \ref{fig:bench-3d},
we provide the relative performance of
Intel MKL and CUSPARSE solvers and compare against
the NEATO shared memory implementation.
The Intel MKL solver can use multiple threads
to solve several systems simultaneously.
The CUSPARSE solver uses a hybrid algorithm composed
of Cyclic Reduction and Parallel Cyclic Reduction (CR+PCR).
The relative performance for
each problem size is obtained by
normalizing the solver timings
by the timing for the fastest solver for that problem size.
Table \ref{table:bench} shows the timings of the various
solvers to solve different problem sizes.

\begin{sidewaystable}[]
\scriptsize
\centering
\caption{Performance of Intel MKL, CUSPARSE and NEATO solvers}
\label{table:bench}
\input{table/bench.tex}
\end{sidewaystable}

%----------------------------------------------------------------------
\section{Application: compact finite difference on multiple GPUs}
\label{sec-compact-finite-differences}
%----------------------------------------------------------------------

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/computational-domain.eps}
\caption{Computational domain in 3-D}
\label{fig:computational-domain}
\end{center}
\end{figure}

\subsection{Numerical algorithm}

In a uniformly spaced one-dimensional grid with spacing $dx$,
if $f_i$ represents the value of
the function evaluated at the $i$th node,
the first derivative $f^{\prime}_i$ can be approximated from
a relation of the form:

\begin{equation}
\begin{split}
    \beta(f^{\prime}_{i-2} + f^{\prime}_{i+2}) + \
    \alpha(f^{\prime}_{i-1} + f^{\prime}_{i+1}) + \
        f^{\prime}_i
    = 
    c\frac{f_{i+3} - f_{i-3}}{dx} + \
    b\frac{f_{i+2} - f_{i-2}}{dx} + \\
    a\frac{f_{i+1} - f_{i-1}}{dx} + \
    \hdots
\end{split}
\label{eqn:general-compact}
\end{equation}

For the particular scheme used, $\alpha = 1/4$,
and $ a = 3/4 $.
It is easily noted that this leads to a tridiagonal system of equations,
along with a 3-point stencil to evaluate the RHS.
Further, the left boundary can be treated using the following implicit equation:

\begin{equation}
    f^{\prime}_1 + 2f^{\prime}_2 = \frac{-5f_1 + 4f_2 + f_3}{dx}
\end{equation}
%
The right boundary ($f^{\prime}_{n}$) is treated by utilizing
the negative complex-conjugate of the Fourier image of the stencil
at $f^{\prime}_1$:

\begin{equation}
    f^{\prime}_{n} + 2f^{\prime}_{n-1s}
    =
    \frac{5f_{n} - 4f_{n-2} 1 f_{n-2}}{dx}
\end{equation}
%
This boundary treatment maintains the tridiagonal identity
of the system to be solved, which then has the form:

\begin{equation} \label{eqn:compact-tridiagonal-system}
 \begin{bmatrix}
     1&2\\
     1/4&1&1/4\\
     &1/4&1&1/4\\
     &&1/4&1&1/4\\
     &&&1/4&1&1/4\\
     &&&&&\ddots\\
     &&&&&&\ddots\\
     &&&&&&&\ddots\\
     &&&&&&&2&1
  \end{bmatrix}
  \begin{bmatrix}
      f^{\prime}_1 \\
      f^{\prime}_2 \\
      f^{\prime}_3 \\
      \vdots \\
      \vdots \\
      \vdots \\
      \vdots \\
      f^{\prime}_{n-1} \\
      f^{\prime}_n
   \end{bmatrix}
 =
 \begin{bmatrix}
     -5f_1 + 4f_2 + f_3\\
     f_{3} - f_{1}\\
     f_{4} - f_{2}\\
     \vdots\\
     \vdots\\
     \vdots\\
     \vdots\\
     f_{n} - f_{n-2}\\
     5f_{n} - 4f_{n-1} - f_{n-2}
  \end{bmatrix}
\end{equation}

\subsection{Computational domain}

The computational domain (Figure \ref{fig:computational-domain})
is a structured grid comprised of
$nx \times ny \times nz$ grid points.
While evaluating the derivative in, say, the x-direction,
the computational domain is thought of as
a collection of \emph{grid lines}---each consisting of
$nx$ grid points---oriented in the x-direction.
For each grid line,
we may assemble a tridiagonal system of size $nx$, as in
Equation \ref{eqn:compact-tridiagonal-system}.
Solving for the all derivatives in the $x$ direction then involves
solving this tridiagonal system for $ny \times nz$ right hand sides.

\subsection{Multi-GPU solver}

For multiple GPUs, we use the algorithm
proposed by Mattor et al. \cite{mattor1995algorithm}.
Here, we distribute the tridiagonal system among
$p$ processors.
We solve for the ``local''  
portion of the global tridiagonal system
by first solving the following three ``local''
tridiagonal systems:

\begin{align}
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
x_{r,1}^p \\
x_{r,2}^p \\
x_{r,3}^p \\
x_{r,4}^p \\
\vdots \\
x_{r,m}^p
\end{bmatrix}
=
\begin{bmatrix}
r_1^p \\
r_2^p \\
r_3^p \\
r_4^p \\
\vdots \\
r_m^p
\end{bmatrix} & \label{eqn:primary-system} \\
%
%
%
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
u_1^p \\
u_2^p \\
u_3^p \\
u_4^p \\
\vdots \\
u_m^p
\end{bmatrix}
=
\begin{bmatrix}
-a_1^p \\
0 \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix} & \label{eqn:secondary-system-1} \\
%
%
%
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
l_1^p \\
l_2^p \\
l_3^p \\
l_4^p \\
\vdots \\
l_m^p
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\vdots \\
-c_m^p
\end{bmatrix} & \label{eqn:secondary-system-2}
\end{align}
%
and additionally, the following ``reduced'' system:

\begin{align} \label{eqn:reduced-system}
&
\begin{bmatrix}
l^1_m & -1 \\
-1    & u^2_1 & l^2_1 \\
      & u^2_m & l^2_m & -1 \\
      &       & -1    & u^3_1 & l^3_1 \\
      &       &       & u^3_m & l^3_n  & -1 \\
      &       &       &       & \ddots & \ddots & \ddots \\
      &       &       &       &        & -1     & u^P_1
\end{bmatrix}
\begin{bmatrix}
\beta^1 \\
\alpha^2 \\
\beta^2 \\
\alpha^3 \\
\beta^3 \\
\vdots \\
\alpha^P
\end{bmatrix}
=
\begin{bmatrix}
x_{r,m}^1 \\
x_{r,1}^2 \\
x_{r,m}^2 \\
x_{r,1}^3 \\
x_{r,m}^3 \\
\vdots \\
x_{r,1}^P \\
\end{bmatrix}
&
\end{align}

We call the system in Equation \ref{eqn:primary-system}
the ``primary'' system, and the systems in
Equations
\ref{eqn:secondary-system-1} and \ref{eqn:secondary-system-2}
the ``secondary'' systems.
The local part of the solution to the ``global'' tridiagonal system
is obtained as a linear combination of
the solutions to the primary and secondary systems:

\begin{equation}
    \boldsymbol{x}^p = \boldsymbol{x}_r^p + \
        \alpha^p \boldsymbol{u}^p + \beta^p \boldsymbol{l}^p
    \label{eqn:sum-of-systems}
\end{equation}

Where $\alpha^p$ and $\beta^p$ are obtained
from the solution of the reduced system.

\subsection{GPU implementation}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=200pt]{img/constructing-reduced-system.eps}
\caption{Construction of the reduced system and scattering of parameters}
\label{fig:reduced-system}
\end{center}
\end{figure}

In this section we describe the implementation of
a CUDA+MPI solver for compact finite differences.
The algorithm 
consists of the following sequence of steps:

\begin{enumerate}
\item Computation of the RHS (requires communication)
\item Solving the primary and secondary systems
\item Constructing and solving the reduced systems,
    and scattering the parameters $\alpha^p$ and $\beta^p$ (requires communication)
\item Summing the above solutions (Equation \ref{eqn:sum-of-systems})
\end{enumerate}

Each of these is performed on the GPU,
and one of the primary concerns is to eliminate
CPU-GPU transfers as much as possible in the solution process.

The right-hand side evaluation is a pointwise stencil computation,
which requires communication of the function values at the boundaries.
This is achieved using a halo-swapping technique
with dedicated contiguous halo arrays for each subdomain,
as communication of non-contiguous MPI
data types is expensive on the GPU.
After the halo information is exchanged,
a pointwise stencil kernel computes the right-hand side values.

The secondary systems are set up and solved only once
as it is dependent only on the
coefficients of the tridiagonal matrix.
As the computation is offline,
any convenient method can be used.
The primary system is solved at every time step
for different right hand sides,
and is evaluated using the NEATO solver.

The set up and solution of the reduced system
requires global communication of the boundary information,
which may be associated with a high overhead.
While several techniques have been proposed to implement
this type of communication pattern efficiently,
we find it sufficient for moderately large problems
to communicate all information to rank 0,
using \texttt{MPI\_Gather}, and distribute the results
back using \texttt{MPI\_Scatter} (Figure \ref{fig:reduced-system}).

Finally, the summing of the solutions
is a pointwise operation that is easily implemented
on the GPU.

For evaluation of compact finite differences in different
coordinate directions,
a local permutation of the data is performed.

\subsection{Results}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=300pt]{fig/strong-scaling-256.eps}
\caption{Strong scaling for multi-GPU compact finite difference, problem size: $256^3$}
\label{fig:strong-scaling-256}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=300pt]{fig/strong-scaling-512.eps}
\caption{Strong scaling for multi-GPU compact finite difference, problem size: $512^3$}
\label{fig:strong-scaling-512}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=300pt]{fig/weak-scaling-128.eps}
\caption{Weak scaling for multi-GPU compact finite difference, problem size: $128^3$}
\label{fig:weak-scaling-128}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=300pt]{fig/weak-scaling-256.eps}
\caption{Weak scaling for multi-GPU compact finite difference, problem size: $256^3$}
\label{fig:weak-scaling-256}
\end{center}
\end{figure}

\pagebreak[4]
\section*{References}

\bibliography{references}
\bibliographystyle{elsarticle-num}
\end{document}
