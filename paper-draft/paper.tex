\documentclass{elsarticle}

\usepackage{amsmath}


\begin{document}

\begin{frontmatter}
\author{Ashwin Srinath}
\title{A}
\maketitle

\begin{abstract}
We present a computation strategy for evaluating
compact finite differences efficiently on GPU clusters.
We describe a novel algorithm for solving the
near-Toeplitz tridiagonal systems associated with
the compact finite difference schemes.
Our specialized tridiagonal solver
takes advantage of the near-Toeplitz nature of the
tridiagonal system to precompute the coefficients
appearing in the Cyclic Reduction algorithm.
This allows us to solve the tridiagonal systems
up to 2 times faster than with the NVIDIA CUSPARSE
gtsvStridedBatch routine.
Additionally, we present a methodology to solve for
compact finite differences fully on multiple GPUs on a cluster
without intermediate host-device transfers.
\end{abstract}

\end{frontmatter}

%----------------------------------------------------------------------%
    
\section{Introduction}

Compact finite difference schemes, developed by
Lele\cite{lele1992compact} 
are extremely popular in the computational fluid dynamics community
for their high order of accuracy at relatively small stencil widths.
These schemes require the solution of
linear systems that are
in general, \emph{banded}, and specifically,
for several popular schemes, \emph{tridiagonal} in nature.
Tridiagonal systems also appear in other numerical schemes
such as
Alternating Direction Implicit (ADI) methods
\cite{1955ADI}
and numerical solutions to one-dimensional differential equations.
In such CFD codes,
the evaluation of these tridiagonal systems constitute
a significant portion of the runtime.
Recently, there has been considerable interest in
the CFD community
in developing tridiagonal solvers
that exploit the massive parallelism afforded by
Graphics Processing Units (GPUs)
to speed up the computations
\cite{tutkun2012gpu}
\cite{esfahanian2014efficient}
\cite{GoSt11CR}.
The performance of tridiagonal solvers on the GPU is
dependent on several factors,
such as
choice of algorithm and the associated complexity,
thread utilization,
use of the available memory heirarchy,
and synchronization and control costs
\cite{Zhang2010FTS}.
The different approaches described in the literature
seek to make tradeoffs between these factors
to arrive at a solver that best fits the requirements.
But to our knowledge, the algorithms proposed so far
are applicable to general tridiagonal systems,
and make no attempt to take advantage of the known structure
of the tridiagonal matrix.
The tridiagonal systems that appear in several numerical schemes
are known to have a \emph{near-Toeplitz} structure.
We describe the implementation of a GPU solver that
makes use of this fact to
reduce the number of computations and
improve memory access.
The resulting solver is thus able to perform twice as fast as
the NVIDIA CUSPARSE equivalent
and $<$add a comparison to the CPU Thomas algorithm$>$.

This paper is organized as follows:

Section \ref{sec:preliminaries}
describes algorithms for solving tridiagonal systems on GPUs.
The cyclic reduction algorithm in particular, is discussed in detail,
and issues with its implementation on GPUs is discussed.
Section \ref{sec:proposed-algorithm}
describes our proposed solver for near-Toeplitz tridiagonal systems.
Section \ref{sec:results-single-gpu}
provides an overview of the performance of our solver,
and compares it with other implementations.

%----------------------------------------------------------------------%
\section{Preliminaries} \label{sec:preliminaries}


\subsection{Solving tridiagonal systems on the GPU}

The classical algorithm for solving tridiagonal systems is
the Thomas algorithm.
The algorithm itself exhibits no parallelism,
and while the GPU can still be exploited to solve
several independent tridiagonal systems,
the performance remains poor.
Zhang et al. \cite{Zhange2010FTS}
describe the implementation of three different algorithms:
cyclic reduction, parallel cyclic reduction, and
recursive doubling,
along with hybrid variants,
on GPUs.
These algorithms perform more work per step than
the Thomas algorithm,
but exhibit more fine-grained parallelism
that allows them to finish in
far fewer than the $2n$ steps required by the Thomas algorithm.
In the best case ($n$ parallel processors),
cyclic reduction requires 
$2log(n) - 1$ steps,
and parallel cyclic reduction requires
$log(n)$ steps
(while doing more work per step than cyclic reduction).
This feature makes them a natural choice for
implementation on GPUs.

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/cyclic-reduction.eps}
\end{center}
\caption{Cyclic reduction - the highlighted elements
show the order in which values are solved for}
\label{fig:cyclic-reduction}
\end{figure}

The Cyclic Reduction algorithm consists of two phases:
\emph{forward reduction} and \emph{backward substitution}
(see Figure \ref{fig:cyclic-reduction}).
The forward substitution phase begins with a system of size $n$,
and by expressing every even-indexed equation $i$ as a linear
combination of equations $i$, $i-1$ and $i+1$, reduces it to a
system of size $n/2$.
The process is repeated until a system of 2 equations in 2 unknowns
is left.

For each even-indexed $i$, we define:

\begin{equation*}
k_1 = \frac{a_i}{b_{i-1}},
k_2 = \frac{c_i}{b_{i+1}}
\end{equation*}

Then, we update the values of $a$, $b$, $c$ and $d$ as:

\begin{align}
& a^{\prime}_i = -a_{i-1}k_1 \
    \label{eqn:forward-reduction-1}& \\
& b^{\prime}_i = b_i - c_{i-1}k_1 - a_{i+1}k_2 \
    \label{eqn:forward-reduction-2}& \\
& c^{\prime}_i = -c_{i+1}k_2 \
    \label{eqn:forward-reduction-3}& \\
& d^{\prime}_i = d_i - d_{i-1}k_1  - d_{i+1}k_2 \
    \label{eqn:forward-reduction-4}&
\end{align}

The 2-by-2 system of equations is solved trivially,
yielding $x_n$ and $\frac{x_n}{2}$.
In the backward substitution phase,
every odd-indexed unknown $x_i$ is solved for by
substituting the known values of $x_{i-1}$ and $x_{i+1}$:

\begin{align} \label{eqn:back-substitution}
x_i = \frac{d^{\prime}_i - a^{\prime}_ix_{i-1} - \
    c^{\prime}_ix_{i+1}}{b^{\prime}_i}
\end{align}

At each step, for the last index $i=n$,
the forward reduction step is instead:
\begin{align} \label{eqn:forward-reduction-last}
    & a^{\prime}_n = -a_{n-1}k_1 & \\
    & b^{\prime}_n = b_n - c_{n-1}k_1 & \\
    & d^{\prime}_n = d_n - d_{n-1}k_1&
\end{align}

And the backward substitution step for $i=1$ is instead:
\begin{align} \label{eqn:back-substitution-first}
x_1 = \frac{d^{\prime}_1 - c^{\prime}_1x_{2}}{b^{\prime}_1}
\end{align}

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/parallel-cyclic-reduction-single-path.eps}
\end{center}
\caption{Parallel cyclic reduction - one reduction path highlighted}
\label{fig:parallel-cyclic-reduction-single-path}
\end{figure}

Parallel cyclic reduction begins by
performing the forward reduction step on
both the odd and even indexed equations.
This results in two systems of size $\frac{n}{2}$ each.
Forward reduction is applied again to each of these systems
resulting in four systems of size $\frac{n}{4}$ each.
The process is repeated till $\frac{n}{2}$ 2-by-2 systems are produced,
which can each be solved trivially.
Figure \ref{fig:parallel-cyclic-reduction-single-path}
shows PCR with one of these ``reduction paths'' highlighted.

Our proposed solver is a specialization of cyclic reduction,
so we discuss the pertinent implementation issues on GPUs.

\subsection{GPU architecture}

Writing applications that effectively exploit the GPU
requires a thorough understanding of the underlying architecure.
We provide the pertinent details here,
and refer to \cite{GPUcomputingera} for a more complete picture.
Our tests are performed on the NVIDIA Tesla K20 and K40 GPUs,
built on the ``Kepler'' architecture,
an overview of which is available at \cite{Keplerwhitepaper}.

Applications that use the GPU include special
pieces of code that are executed on the GPU,
referred to as \emph{kernels}.
In C, for example, kernels are written as functions,
and are called with (almost) the same conventions.
A kernel is executed in parallel by several threads,
which are organized into a \emph{grid} of \emph{thread blocks}
(or just \emph{blocks}).
As a requirement, thread blocks must be able to run
independent of each other,
in series or in parallel.

From the hardware perspective, an NVIDIA GPU may be viewed primarily as
a collection of so-called Streaming Microprocessors (SMs).
When a kernel is launched (with a specified number of thread blocks),
each block is assigned to an SM.
Threads within a thread block can execute concurrently on the SM,
and an SM can execute several thread blocks concurrently.
The number of thread blocks that an SM can execute concurrently
is limited by a number of factors,
and maximizing this number is often key to obtaining good performance.

Every thread in every block has access to so-called \emph{global memory},
which can also be accessed by the CPU (or \emph{host}) via the PCI-e bus.
Further,
threads within a thread block have access
to a common, fast, limited \emph{shared memory}.
The contents of shared memory are managed by the kernel code,
and shared memory is often viewed as an explicitly controlled cache.
Each thread also has private \emph{local memory},
and access to extremely fast registers.
Unlike CPUs, the GPU has a large number of registers---a
thread executing a kernel will
typically attempt to store the variables defined in registers,
before spilling over to local memory, which is much slower.

Thread access to global memory is slow,
and is especially inefficient when
successive threads in a block
access locations that are far apart in memory---known
as \emph{uncoalesced} memory access.
Shared memory access can be expected to be
much faster than global memory---however,
the amount of shared memory per SM is limited
(48 KiB for current GPUs).
The amount of shared memory actually used by each block,
can thus limit the number of blocks that concurrently run on each SM.
The number of registers per SM is also limited,
and the use of registers can similarly limit
the number of concucrrently running blocks.
Finally, memory transfers between the CPU (host) and GPU (device)
are extremely expensive, limited by the bandwidth of the PCI-e bus.

\subsection{Cyclic reduction on GPU}




%----------------------------------------------------------------------%
\section{Proposed algorithm} \label{sec:proposed-algorithm}

We note that solving the equations
\ref{eqn:forward-reduction-1}-\ref{eqn:forward-reduction-3}
for each grid line and at each time step of the simulation
is wasteful.
It may be benefitial to
\emph{precompute} and store the coefficients appearing at
each step of the forward reduction,
and to load these values from memory when required.

\subsection{General tridiagonal matrix}

For a general tridiagonal matrix,
in addition to the $3n$ original coefficient arrays
$a$, $b$ and $c$,
this requires extra storage of 
$\frac{n}{2}+\frac{n}{4}+\frac{n}{8}+...=n$
elements each
for $a_i$, $b_i$, $c_i$, $k1$ and $k2$
appearing at the forward reduction steps.
This additional storage cost is greatly amortized
when solving the same system for multiple right-hand sides.
But the additional arrays
still need to be loaded into shared memory for each thread block.
This raises the shared memory storage from $4n$ to $9n$,
which significantly impacts the number of
concurrently running blocks.

\subsection{Near-Toeplitz matrix}

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/cyclic-reduction-boundaries.eps}
\end{center}
\caption{Indices affected by boundaries in cyclic reduction}
\label{fig:cyclic-reduction-boundaries}
\end{figure}

Precomputing the forward reduction coefficients
becomes especially effective for near-Toeplitz matrices.
At each reduction step,
the coefficients $a_i$, $b_i$, $c_i$, $k1_i$ and $k2_i$ 
are the same for all indices $i$,
except for indices ``polluted'' by the boundary conditions
(Figure \ref{fig:cyclic-reduction-boundaries}).
Ignoring the boundaries, then, we find that the precomputed coefficients
require $5(log_2(n) - 1)$ storage---far lower than the $5n$ required for the
general case.
Further, a careful analysis of the equations
\ref{eqn:forward-reduction-1}-\ref{eqn:forward-reduction-3}
reveals that the following extra arrays are required
to store the boundary information:

\begin{enumerate}
    \item The values of $b_1$ at each reduction step
    \item The values of $k_{1,1}$ at each reduction step
    \item The values of $k_{1,n}$ at each reduction step
    \item The values of $a_n$ and $b_n$ at the final reduction step
        (required for the 2-by-2 solve)
\end{enumerate}

This puts the total storage for precomputed coefficients
at $8(log_2(n) - 1)+2$.

%----------------------------------------------------------------------%
\section{Results: single GPU} \label{sec:results-single-gpu}








%----------------------------------------------------------------------%

For example,
in a uniformly spaced one-dimensional grid with spacing $dx$,
if $f_i$ represents the value of
the function evaluated at the $i$th node,
the first derivative $f^{\prime}_i$ can be approximated from
a relation of the form:

\begin{equation}
\begin{split}
    \beta(f^{\prime}_{i-2} + f^{\prime}_{i+2}) + \
    \alpha(f^{\prime}_{i-1} + f^{\prime}_{i+1}) + \
        f^{\prime}_i
    = 
    c\frac{f_{i+3} - f_{i-3}}{dx} + \
    b\frac{f_{i+2} - f_{i-2}}{dx} + \\
    a\frac{f_{i+1} - f_{i-1}}{dx} + \
    \hdots
\end{split}
\label{eqn:general-compact}
\end{equation}

The derivatives near the boundaries are approximated using
one-sided stencil operators of the form:

\begin{equation}
f^{\prime}_1 + \alpha_1 f^{\prime}_2 = \
    \frac{1}{dx} (a_1 f_1 + b_1 f_2 + c_1 f_3 + d_1 f_4) 
\end{equation}

and 

\begin{equation}
    f^{\prime}_n + \alpha_n f^{\prime}_{n-1} = \
    \frac{1}{dx} (a_n f_n + b_n f_{n-1} + c_n f_{n-2} + d_n f_{n-3}) 
\end{equation}

As described by Lele \cite{lele1992compact},
setting $\beta = 0$ in Equation \ref{eqn:general-compact} leads to
a family of tridiagonal schemes with a single parameter ($\alpha$).
The coefficient matrix associated with such tridiagonal schemes
has the general form:

\begin{equation} \label{eqn:compact-tridiagonal-system}
\begin{bmatrix}
     1 &  \alpha_1  \\
     \alpha   &  1   &  \alpha \\
         &  \alpha   &  1  &  \alpha  \\
         &      &  \alpha  &  1  &  \alpha  \\
         &      &     &     &  \ddots \\
         &      &     &     &     &  \ddots  \\
         &      &     &     &     &  \alpha_n &  1
\end{bmatrix}
\begin{bmatrix}
    f^{\prime}_1 \\
    f^{\prime}_2 \\
    f^{\prime}_3 
    \vdots \\
    \vdots \\
    f^{\prime}_{n-1} \\
    f^{\prime}_n
 \end{bmatrix}
=
\begin{bmatrix}
   d_1 \\
   d_2 \\
   d_3 \\
   \vdots \\
   \vdots \\
   d_{n-1} \\
   d_{n}
\end{bmatrix}
\end{equation}

We note that the system is \emph{near Toeplitz tridiagonal},
i.e., the left-hand-side matrix is tridiagonal with
constant coefficients along each diagonal,
except for the first and last equations.

%----------------------------------------------------------------------%

\begin{figure}[h!]
\begin{center}
\includegraphics[height=200pt]{img/computational-domain.eps}
\end{center}
\caption{Computational domain in 3-D}
\label{fig:computational-domain}
\end{figure}

The computational domain (Figure \ref{fig:computational-domain})
is a structured grid comprised of $nx \times ny \times nz$ grid points.
While evaluating the derivative in, say, the x-direction,
the computational domain is thought of as
a collection of \emph{grid lines}---each consisting of
$nx$ grid points---oriented in the x-direction.
For each grid line, we may assemble the system in
Equation \ref{eqn:compact-tridiagonal-system}.
Solving for the derivatives in the $x$ direction
at each grid point in the domain then entails
solving a tridiagonal system for each of the $ny \times nz$ grid lines.
We note immediately that the left-hand-side (LHS) matrix
is the same for each of the tridiagonal systems,
and only the right-hand-side (RHS) vector is different.

%----------------------------------------------------------------------%

The main step in evaluating compact-finite differences is the
solution of tridiagonal systems
for each grid line in the computational domain.
The standard approach for solving tridiagonal systems is
the Thomas algorithm.
However, on parallel architectures such as GPUs,
it is advantageous to use other algorithms like
cyclic reduction \cite{Zhang2010FTS}.

%----------------------------------------------------------------------%


%----------------------------------------------------------------------%

We note from
Equations \ref{eqn:forward-reduction-1}-\ref{eqn:forward-reduction-3}
that each forward reduction step involves 12 operations.
As the LHS matrix is constant,
it is advantageous to pre-compute the coefficients appearing
in the forward reduction step.
For a general tridiagonal matrix,
in addition to the $3n$ original coefficient arrays
$a$, $b$ and $c$,
this requires extra storage of 
$3(\frac{n}{2}+\frac{n}{4}+\frac{n}{8}+...)=3n$
coefficients.
This storage cost is greatly amortized
when solving multiple right-hand sides.

We use the algorithm discussed by Mattor et al.
\cite{mattor1995algorithm}
to distribute and solve the problem among multiple processes. 
We present the algorithm to solve a general tridiagonal system,
and in subsequent sections discuss its adaptation to solving
near-Teoplitz tridiagonal systems on GPUs.

Given a tridiagonal system with  $n$ equations,
to be solved by $P$ processes:

\begin{align}
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
x_{r,1}^p \\
x_{r,2}^p \\
x_{r,3}^p \\
x_{r,4}^p \\
\vdots \\
x_{r,m}^p
\end{bmatrix}
=
\begin{bmatrix}
r_1^p \\
r_2^p \\
r_3^p \\
r_4^p \\
\vdots \\
r_m^p
\end{bmatrix} & \label{eqn:global-system} 
\end{align}

each process $p$ proceeds by solving the following three ``local''
tridiagonal systems:

\begin{align}
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
x_{r,1}^p \\
x_{r,2}^p \\
x_{r,3}^p \\
x_{r,4}^p \\
\vdots \\
x_{r,m}^p
\end{bmatrix}
=
\begin{bmatrix}
r_1^p \\
r_2^p \\
r_3^p \\
r_4^p \\
\vdots \\
r_m^p
\end{bmatrix} & \label{eqn:primary-system} \\
%
%
%
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
u_1^p \\
u_2^p \\
u_3^p \\
u_4^p \\
\vdots \\
u_m^p
\end{bmatrix}
=
\begin{bmatrix}
-a_1^p \\
0 \\
0 \\
0 \\
\vdots \\
0
\end{bmatrix} & \label{eqn:secondary-system-1} \\
%
%
%
& \begin{bmatrix}
b_1^p & c_1^p \\
a_2^p & b_2^p & c_2^p \\
      & a_3^p & b_3^p & c_3^p \\
      &       & a_4^p & b_4^p & c_4^p \\
      &       &       &       &  \ddots & c_{m-1}^p\\
      &       &       &       &     a_{m}^p  & b_{m}^p
\end{bmatrix}
\begin{bmatrix}
l_1^p \\
l_2^p \\
l_3^p \\
l_4^p \\
\vdots \\
l_m^p
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\vdots \\
-c_m^p
\end{bmatrix} & \label{eqn:secondary-system-2}
\end{align}

We call the system in Equation \ref{eqn:primary-system}
the ``primary'' system, and the systems in
Equations
\ref{eqn:secondary-system-1} and \ref{eqn:secondary-system-2}
the ``secondary'' systems.
The local part of the solution to the ``global'' tridiagonal system
(Equation \ref{eqn:global-system})
is obtained as a linear combination of
the solutions to the primary and secondary sytems:

\begin{equation}
    \boldsymbol{x}^p = \boldsymbol{x}_r^p + \
        \alpha^p \boldsymbol{u}^p + \beta^p \boldsymbol{l}^p
    \label{eqn:sum-of-systems}
\end{equation}

where the  parameters $\alpha^p$ and $\beta^p$ are obtained by
solving the following ``reduced'' system of equations:

\begin{equation} \label{eqn:reduced-system}
\begin{bmatrix}
l^1_m & -1 \\
-1    & u^2_1 & l^2_1 \\
      & u^2_m & l^2_m & -1 \\
      &       & -1    & u^3_1 & l^3_1 \\
      &       &       & u^3_m & l^3_n  & -1 \\
      &       &       &       & \ddots & \ddots & \ddots \\
      &       &       &       &        & -1     & u^P_1
\end{bmatrix}
\begin{bmatrix}
\beta^1 \\
\alpha^2 \\
\beta^2 \\
\alpha^3 \\
\beta^3 \\
\vdots \\
\alpha^P
\end{bmatrix}
=
\begin{bmatrix}
x_{r,m}^1 \\
x_{r,1}^2 \\
x_{r,m}^2 \\
x_{r,1}^3 \\
x_{r,m}^3 \\
\vdots \\
x_{r,1}^P \\
\end{bmatrix}
\end{equation}

$P = n/m$ is the number of processes.
We note that the reduced system is sized $2P-2$,
and that its assembly requires communication between the processes.
The general solution procedure is then:

\begin{enumerate}
    \item Each process $p$ solves the primary and secondary systems
        to obtain $\boldsymbol{x}_r^p$, $u^p$ and $l^p$
    \item The ``reduced'' system is assembled and solved for
        the coefficients $\alpha^p$ and $\beta^p$
    \item The local solution is computed from Equation \ref{eqn:sum-of-systems}
\end{enumerate}


\section*{References}

\bibliography{references}
\bibliographystyle{elsarticle-num}
\end{document}
